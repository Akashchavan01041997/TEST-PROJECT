# -*- coding: utf-8 -*-
"""nlped4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p-kvaZjZHCyEcVTXRzvqRRIKOSbliX_W
"""

import pandas as pd
import nltk
import re
import numpy as np
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.impute import SimpleImputer  # For handling missing values
from imblearn.over_sampling import SMOTE  # For handling class imbalance

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# 1. Data Loading and Preprocessing
df = pd.read_csv('/content/train.csv')

# Handling missing values with imputation
imputer = SimpleImputer(strategy='most_frequent')  # Replace with your preferred strategy
df[['keyword', 'location']] = imputer.fit_transform(df[['keyword', 'location']])

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    if isinstance(text, str):
        text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove non-alphanumeric characters
        text = text.lower()
        tokens = nltk.word_tokenize(text)
        tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]  # Lemmatization
        return tokens
    else:
        return []

df['processed_text'] = df['text'].apply(preprocess_text)

# 2. Word2Vec Embedding with Hyperparameter Tuning
corpus = df['processed_text'].tolist()

# Hyperparameter tuning for Word2Vec (example)
word2vec_params = {
    'vector_size': [100, 200, 300],
    'window': [5, 7, 9],
    'min_count': [1, 3, 5]
}

# Use GridSearchCV to find the best hyperparameters for Word2Vec
# ... (Code for GridSearchCV with Word2Vec) ...

# After tuning, create the Word2Vec model with the best parameters
model = Word2Vec(corpus, vector_size=200, window=7, min_count=3, workers=4, sg=1)  # Replace with best parameters

# 3. Feature Extraction
def get_document_vector(tokens, model):
    vectors = [model.wv[token] for token in tokens if token in model.wv]
    if vectors:
        return np.mean(vectors, axis=0)
    else:
        return np.zeros(model.vector_size)

df['document_vector'] = df['processed_text'].apply(lambda tokens: get_document_vector(tokens, model))

# 4. Model Training and Evaluation
X = np.array(df['document_vector'].tolist())
y = df['target']

# Addressing class imbalance with SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Hyperparameter tuning for RandomForestClassifier (example)
rf_params = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10]
}

# Use GridSearchCV to find the best hyperparameters for RandomForestClassifier
# ... (Code for GridSearchCV with RandomForestClassifier) ...

# After tuning, create the RandomForestClassifier model with the best parameters
classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # Replace with best parameters
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

# Evaluate performance with multiple metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-score: {f1}")


# Example new tweet for prediction:
new_tweet = "There's a huge fire near my house, and people are evacuating!"

# Preprocess the new tweet:
processed_tweet = preprocess_text(new_tweet)
tweet_vector = get_document_vector(processed_tweet, model)

# Make the prediction:
prediction = classifier.predict([tweet_vector])[0]  # Get the prediction (0 or 1)

# Print the prediction:
if prediction == 1:
    print(f"Tweet: '{new_tweet}' is predicted as a real disaster.")
else:
    print(f"Tweet: '{new_tweet}' is predicted as a fake disaster.")

# Load unseen data
unseen_df = pd.read_csv('test.csv')  # Replace 'unseen_data.csv' with your file

# Preprocess unseen data
unseen_df['processed_text'] = unseen_df['text'].apply(preprocess_text)
unseen_df['document_vector'] = unseen_df['processed_text'].apply(lambda tokens: get_document_vector(tokens, model))

# Make predictions on unseen data
X_unseen = np.array(unseen_df['document_vector'].tolist())
unseen_predictions = classifier.predict(X_unseen)

# Add predictions to the unseen data DataFrame
unseen_df['prediction'] = unseen_predictions

# Save the predictions or print them
unseen_df.to_csv('unseen_data_with_predictions.csv', index=False)  # Save to a new CSV
# Or print the predictions:
# for index, row in unseen_df.iterrows():
#     print(f"Tweet: {row['text']}, Prediction: {row['prediction']}")

import pandas as pd
import nltk
import re
import numpy as np
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer  # Import PorterStemmer
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Download necessary NLTK resources
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()  # Initialize PorterStemmer

def preprocess_text(text):
    if isinstance(text, str):
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        text = text.lower()
        tokens = nltk.word_tokenize(text)
        tokens = [stemmer.stem(token) for token in tokens]  # Apply stemming
        tokens = [token for token in tokens if token not in stop_words]
        return tokens
    else:
        return []

df = pd.read_csv('/content/train.csv')
df.fillna(value=np.nan, inplace=True)
df['processed_text'] = df['text'].apply(preprocess_text)

# 1. Word2Vec Embedding with Hyperparameter Tuning:
corpus = df['processed_text'].tolist()
model = Word2Vec(corpus, vector_size=200, window=7, min_count=3, workers=4, sg=1)

# 2. Feature Extraction (Averaging Word Vectors):
def get_document_vector(tokens, model):
    vectors = [model.wv[token] for token in tokens if token in model.wv]
    if vectors:
        return np.mean(vectors, axis=0)
    else:
        return np.zeros(model.vector_size)

df['document_vector'] = df['processed_text'].apply(lambda tokens: get_document_vector(tokens, model))

# 3. Model Training (RandomForestClassifier):
X = np.array(df['document_vector'].tolist())
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

classifier = RandomForestClassifier(n_estimators=100, random_state=42)
classifier.fit(X_train, y_train)

# 4. Prediction and Evaluation:
y_pred = classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Example Prediction:
new_text = "Example of a new tweet to predict"
processed_new_text = preprocess_text(new_text)
new_text_vector = get_document_vector(processed_new_text, model)
prediction = classifier.predict([new_text_vector])

if prediction[0] == 1:
    print(f"Tweet: '{new_text}' is predicted as a real disaster.")
else:
    print(f"Tweet: '{new_text}' is predicted as a fake disaster.")



import pandas as pd
import nltk
import re
import numpy as np
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer  # Import PorterStemmer
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Download necessary NLTK resources
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()  # Initialize PorterStemmer

def preprocess_text(text):
    if isinstance(text, str):
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        text = text.lower()
        tokens = nltk.word_tokenize(text)
        tokens = [stemmer.stem(token) for token in tokens]  # Apply stemming
        tokens = [token for token in tokens if token not in stop_words]
        return tokens
    else:
        return []

df = pd.read_csv('/content/train.csv')
df.fillna(value=np.nan, inplace=True)
df['processed_text'] = df['text'].apply(preprocess_text)

# 1. Word2Vec Embedding with Hyperparameter Tuning:
corpus = df['processed_text'].tolist()
model = Word2Vec(corpus, vector_size=200, window=7, min_count=3, workers=4, sg=1)

# 2. Feature Extraction (Averaging Word Vectors):
def get_document_vector(tokens, model):
    vectors = [model.wv[token] for token in tokens if token in model.wv]
    if vectors:
        return np.mean(vectors, axis=0)
    else:
        return np.zeros(model.vector_size)

df['document_vector'] = df['processed_text'].apply(lambda tokens: get_document_vector(tokens, model))

# 3. Model Training (RandomForestClassifier):
X = np.array(df['document_vector'].tolist())
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

classifier = RandomForestClassifier(n_estimators=100, random_state=42)
classifier.fit(X_train, y_train)

# 4. Prediction and Evaluation:
y_pred = classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Example Prediction:
new_text = "Example of a new tweet to predict"
processed_new_text = preprocess_text(new_text)
new_text_vector = get_document_vector(processed_new_text, model)
prediction = classifier.predict([new_text_vector])

if prediction[0] == 1:
    print(f"Tweet: '{new_text}' is predicted as a real disaster.")
else:
    print(f"Tweet: '{new_text}' is predicted as a fake disaster.")

import pandas as pd
import nltk
import re
import numpy as np
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer  # Import PorterStemmer
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Download necessary NLTK resources
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()  # Initialize PorterStemmer

def preprocess_text(text):
    if isinstance(text, str):
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        text = text.lower()
        tokens = nltk.word_tokenize(text)
        tokens = [stemmer.stem(token) for token in tokens]  # Apply stemming
        tokens = [token for token in tokens if token not in stop_words]
        return tokens
    else:
        return []

df = pd.read_csv('/content/train.csv')
df.fillna(value=np.nan, inplace=True)
df['processed_text'] = df['text'].apply(preprocess_text)

# 1. Word2Vec Embedding with Hyperparameter Tuning:
corpus = df['processed_text'].tolist()
model = Word2Vec(corpus, vector_size=200, window=7, min_count=3, workers=4, sg=1)

# 2. Feature Extraction (Averaging Word Vectors):
def get_document_vector(tokens, model):
    vectors = [model.wv[token] for token in tokens if token in model.wv]
    if vectors:
        return np.mean(vectors, axis=0)
    else:
        return np.zeros(model.vector_size)

df['document_vector'] = df['processed_text'].apply(lambda tokens: get_document_vector(tokens, model))

# 3. Model Training (RandomForestClassifier):
X = np.array(df['document_vector'].tolist())
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

classifier = RandomForestClassifier(n_estimators=100, random_state=42)
classifier.fit(X_train, y_train)

# 4. Prediction and Evaluation:
y_pred = classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Example Prediction:
new_text = "Example of a new tweet to predict"
processed_new_text = preprocess_text(new_text)
new_text_vector = get_document_vector(processed_new_text, model)
prediction = classifier.predict([new_text_vector])

if prediction[0] == 1:
    print(f"Tweet: '{new_text}' is predicted as a real disaster.")
else:
    print(f"Tweet: '{new_text}' is predicted as a fake disaster.")

import pandas as pd
import nltk
import re
import numpy as np
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
# Importing RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Download necessary NLTK resources
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    if isinstance(text, str):
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        text = text.lower()
        tokens = nltk.word_tokenize(text)
        tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]
        return tokens
    else:
        return []

df = pd.read_csv('/content/train.csv')
df.fillna(value=np.nan, inplace=True)
df['processed_text'] = df['text'].apply(preprocess_text)

# 1. Word2Vec Embedding with Hyperparameter Tuning:
corpus = df['processed_text'].tolist()
model = Word2Vec(corpus, vector_size=200, window=7, min_count=3, workers=4, sg=1) #Adjust parameters

# 2. Feature Extraction (Averaging Word Vectors):
def get_document_vector(tokens, model):
    vectors = [model.wv[token] for token in tokens if token in model.wv]
    if vectors:
        return np.mean(vectors, axis=0)  # Using NumPy for averaging
    else:
        return np.zeros(model.vector_size)

df['document_vector'] = df['processed_text'].apply(lambda tokens: get_document_vector(tokens, model))

# 3. Model Training (RandomForestClassifier):
X = np.array(df['document_vector'].tolist()) # Convert to NumPy array
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # Using RandomForestClassifier
classifier.fit(X_train, y_train)

# 4. Prediction and Evaluation:
y_pred = classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Example Prediction:
new_text = "	All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected"
processed_new_text = preprocess_text(new_text)
new_text_vector = get_document_vector(processed_new_text, model)
prediction = classifier.predict([new_text_vector])

if prediction[0] == 1:
    print(f"Tweet: '{new_text}' is predicted as a real disaster.")
else:
    print(f"Tweet: '{new_text}' is predicted as a fake disaster.")

!pip install xgboost
import pandas as pd
import nltk
import re
import numpy as np
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
# Import TF-IDF Vectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
# Import XGBoost
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

# Download necessary NLTK resources
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    if isinstance(text, str):
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        text = text.lower()
        tokens = nltk.word_tokenize(text)
        tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]
        return ' '.join(tokens)  # Join tokens back into a string for TF-IDF
    else:
        return ''

df = pd.read_csv('/content/train.csv')
df.fillna(value=np.nan, inplace=True)
df['processed_text'] = df['text'].apply(preprocess_text)

# 1. TF-IDF Feature Extraction:
vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed
X = vectorizer.fit_transform(df['processed_text'])

# 2. Model Training (XGBoost):
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

classifier = XGBClassifier(random_state=42) # using XGBoost Classifier
classifier.fit(X_train, y_train)

# 3. Prediction and Evaluation:
y_pred = classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Example Prediction:
new_text = "All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected"
processed_new_text = preprocess_text(new_text)
new_text_vector = vectorizer.transform([processed_new_text])  # Transform using TF-IDF
prediction = classifier.predict(new_text_vector)

if prediction[0] == 1:
    print(f"Tweet: '{new_text}' is predicted as a real disaster.")
else:
    print(f"Tweet: '{new_text}' is predicted as a fake disaster.")

import pandas as pd
import nltk
import re
import numpy as np
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Download the missing 'punkt_tab' resource:
nltk.download('punkt_tab')

# 1. Data Loading and Preprocessing
df = pd.read_csv('/content/train.csv')  # Using your provided dataset path

# Handle missing values (replace with your preferred method)
df.fillna(value=np.nan, inplace=True)
# Or use df.dropna() to remove rows with missing values

nltk.download('stopwords')
nltk.download('wordnet')
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    if isinstance(text, str):
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        text = text.lower()
        tokens = nltk.word_tokenize(text)
        tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]
        return tokens
    else:
        return []

df['processed_text'] = df['text'].apply(preprocess_text)

# 2. Word2Vec Embedding
corpus = df['processed_text'].tolist()
model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, workers=4)

# 3. Feature Extraction
def get_document_vector(tokens, model):
    vectors = [model.wv[token] for token in tokens if token in model.wv]
    if vectors:
        return sum(vectors) / len(vectors)
    else:
        return [0] * model.vector_size

df['document_vector'] = df['processed_text'].apply(lambda tokens: get_document_vector(tokens, model))

# 4. Model Training and Prediction
X = df['document_vector'].to_list()
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

classifier = LogisticRegression()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Example Prediction
new_text = "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all"
processed_new_text = preprocess_text(new_text)
new_text_vector = get_document_vector(processed_new_text, model)
prediction = classifier.predict([new_text_vector])

if prediction[0] == 1:
    print(f"Tweet: '{new_text}' is predicted as a real disaster.")
else:
    print(f"Tweet: '{new_text}' is predicted as a fake disaster.")

#NLP project on disaster tweets by using nltk and word2vec
#load the train dataset
import pandas as pd
df=pd.read_csv('train.csv')
df.head()

import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('wordnet')

nltk.download('stopwords')

print(stopwords.words('english'))

import pandas as pd
df=pd.read_csv('train.csv')

#data preprocessing
df=df.drop(['id','keyword','location'],axis=1)
df.head()

#check null values in data
df.isnull().sum()

df.shape

#separating the data and lables
x=df['text'].values
y=df['target'].values

print(x)
print(y)

#stemming:Stemming is the process of reducing a word to its root word
import nltk
from nltk.stem.porter import PorterStemmer
porter_stemmer =PorterStemmer()

#crating function for stemming
def stemming(content):
    stemmed_content=re.sub('[^a-zA-Z]',' ',content)
    stemmed_content=stemmed_content.lower()
    stemmed_content=stemmed_content.split()
    stemmed_content=[porter_stemmer.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
    stemmed_content=' '.join(stemmed_content)
    return stemmed_content

#now stemming apply on feature means on x
import re
# Convert x back to a pandas Series to use 'apply' with column names
x = pd.Series(x)
x = x.apply(stemming)

#use here word2vect for word embedding
from gensim.models import Word2Vec

#now use word2vect here
import gensim
from gensim.models import Word2Vec

#now x is our pre prossed data and stemmed data
corpus = []
for i in range(len(x)):
    corpus.append(x[i].split())

#train the word2vect model
model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, workers=4)

#Now split data for further task
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,stratify=y,random_state=2)

