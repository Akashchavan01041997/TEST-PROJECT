{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkJljY1By+/peVGgoCioY/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akashchavan01041997/TEST-PROJECT/blob/main/disasterfinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pandas nltk scikit-learn imbalanced-learn gensim textblob\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from imblearn.combine import SMOTETomek\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.downloader import load\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# 1. Data Loading and Preprocessing\n",
        "df = pd.read_csv('/content/train.csv')\n",
        "\n",
        "# Handling missing values\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "df[['keyword', 'location']] = imputer.fit_transform(df[['keyword', 'location']])\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(['rt', 'amp', 'via'])  # Add custom stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
        "        text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
        "        text = re.sub(r'[^a-zA-Z\\s#]', '', text)  # Keep hashtags\n",
        "        text = text.lower()\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "        return tokens\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Add new features\n",
        "df['text_length'] = df['text'].apply(len)\n",
        "df['num_hashtags'] = df['text'].apply(lambda x: x.count('#'))\n",
        "df['sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "\n",
        "# 2. Word2Vec Embedding with Pre-trained Model\n",
        "# Load pre-trained Word2Vec model (glove-twitter-25)\n",
        "#model = Word2Vec(df['processed_text'].tolist(), vector_size=200, window=5, min_count=2)\n",
        "#model.save(\"word2vec.model\")\n",
        "#model = Word2Vec.load(\"word2vec.model\")\n",
        "#model = api.load('word2vec-google-news-300')\n",
        "model = load('glove-twitter-25', return_path=False)\n",
        "\n",
        "# 3. Feature Extraction\n",
        "def get_document_vector(tokens, model):\n",
        "    vectors = [model[token] for token in tokens if token in model]\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "df['document_vector'] = df['processed_text'].apply(lambda tokens: get_document_vector(tokens, model))\n",
        "\n",
        "# Combine new features with document vector\n",
        "additional_features = df[['text_length', 'num_hashtags', 'sentiment']].to_numpy()\n",
        "X_vectors = np.array(df['document_vector'].tolist())\n",
        "X_combined = np.hstack((X_vectors, additional_features))\n",
        "\n",
        "y = df['target']\n",
        "\n",
        "# Addressing class imbalance with SMOTETomek\n",
        "smt = SMOTETomek(random_state=42)\n",
        "X_resampled, y_resampled = smt.fit_resample(X_combined, y)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Model Training with RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "y_pred_proba = classifier.predict_proba(X_test)[:, 1]  # For ROC-AUC\n",
        "\n",
        "# Evaluate performance with multiple metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n",
        "print(f\"ROC-AUC: {roc_auc}\")\n",
        "\n",
        "# 5. Predicting New Tweets\n",
        "new_tweet = \"Forest fire near La Ronge Sask. Canada\"\n",
        "\n",
        "# Preprocess the new tweet\n",
        "processed_tweet = preprocess_text(new_tweet)\n",
        "tweet_vector = get_document_vector(processed_tweet, model)\n",
        "\n",
        "# Add additional features for the new tweet\n",
        "text_length = len(new_tweet)\n",
        "num_hashtags = new_tweet.count('#')\n",
        "sentiment = TextBlob(new_tweet).sentiment.polarity\n",
        "new_tweet_combined = np.hstack((tweet_vector, [text_length, num_hashtags, sentiment]))\n",
        "\n",
        "# Make the prediction\n",
        "prediction = classifier.predict([new_tweet_combined])[0]\n",
        "\n",
        "# Print the prediction\n",
        "if prediction == 1:\n",
        "    print(f\"Tweet: '{new_tweet}' is predicted as a real disaster.\")\n",
        "else:\n",
        "    print(f\"Tweet: '{new_tweet}' is predicted as a fake disaster.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJctWypjOPUP",
        "outputId": "cad2dc73-5ea8-409a-80b5-3b284625ed1b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTETomek or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py:354: FutureWarning: The TomekLinks or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8206686930091185\n",
            "Precision: 0.8402061855670103\n",
            "Recall: 0.7922235722964763\n",
            "F1-score: 0.815509693558474\n",
            "ROC-AUC: 0.8868332283823055\n",
            "Tweet: 'Forest fire near La Ronge Sask. Canada' is predicted as a real disaster.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Importing Libraries and Resources"
      ],
      "metadata": {
        "id": "uxRRL5pv3vAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.impute import SimpleImputer  # For handling missing values\n",
        "from imblearn.over_sampling import SMOTE  # For handling class imbalance\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFhBs0TO10PC",
        "outputId": "67eb1a31-9ef9-41de-8009-8238b96d9592"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "37tu87wl35is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Data Loading and Preprocessing\n",
        "df = pd.read_csv('/content/train.csv')\n",
        "\n",
        "# Handling missing values with imputation\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # Replace with your preferred strategy\n",
        "df[['keyword', 'location']] = imputer.fit_transform(df[['keyword', 'location']])\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphanumeric characters\n",
        "        text = text.lower()\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]  # Lemmatization\n",
        "        return tokens\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "df['processed_text'] = df['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "I15LqY0x2wcC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Word2Vec Embedding with Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "88HpenGb4Eh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Word2Vec Embedding with Hyperparameter Tuning\n",
        "corpus = df['processed_text'].tolist()\n",
        "\n",
        "# Hyperparameter tuning for Word2Vec (example)\n",
        "word2vec_params = {\n",
        "    'vector_size': [100, 200, 300],\n",
        "    'window': [5, 7, 9],\n",
        "    'min_count': [1, 3, 5]\n",
        "}\n",
        "\n",
        "# Use GridSearchCV to find the best hyperparameters for Word2Vec\n",
        "# ... (Code for GridSearchCV with Word2Vec) ...\n",
        "\n",
        "# After tuning, create the Word2Vec model with the best parameters\n",
        "model = Word2Vec(corpus, vector_size=200, window=7, min_count=3, workers=4, sg=1)  # Replace with best parameters"
      ],
      "metadata": {
        "id": "Zexqngu_281X"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Feature Extraction"
      ],
      "metadata": {
        "id": "iAIcrTOa4NnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Feature Extraction\n",
        "def get_document_vector(tokens, model):\n",
        "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "df['document_vector'] = df['processed_text'].apply(lambda tokens: get_document_vector(tokens, model))"
      ],
      "metadata": {
        "id": "EvW4HAHs3FP2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Model Training and Evaluation"
      ],
      "metadata": {
        "id": "4y36TIVF4dcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Model Training and Evaluation\n",
        "X = np.array(df['document_vector'].tolist())\n",
        "y = df['target']\n",
        "\n",
        "# Addressing class imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Hyperparameter tuning for RandomForestClassifier (example)\n",
        "rf_params = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 5, 10]\n",
        "}\n",
        "\n",
        "# Use GridSearchCV to find the best hyperparameters for RandomForestClassifier\n",
        "# ... (Code for GridSearchCV with RandomForestClassifier) ...\n",
        "\n",
        "# After tuning, create the RandomForestClassifier model with the best parameters\n",
        "classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # Replace with best parameters\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate performance with multiple metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vawyBZuP3RyB",
        "outputId": "d4d47597-5235-4468-a9d4-61136e782a98"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7829591249280369\n",
            "Precision: 0.8161209068010076\n",
            "Recall: 0.7372013651877133\n",
            "F1-score: 0.7746563060370592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Example Prediction"
      ],
      "metadata": {
        "id": "pPdBieGP4t4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example new tweet for prediction:\n",
        "new_tweet = \"There's a huge fire near my house, and people are evacuating!\"\n",
        "\n",
        "# Preprocess the new tweet:\n",
        "processed_tweet = preprocess_text(new_tweet)\n",
        "tweet_vector = get_document_vector(processed_tweet, model)\n",
        "\n",
        "# Make the prediction:\n",
        "prediction = classifier.predict([tweet_vector])[0]  # Get the prediction (0 or 1)\n",
        "\n",
        "# Print the prediction:\n",
        "if prediction == 1:\n",
        "    print(f\"Tweet: '{new_tweet}' is predicted as a real disaster.\")\n",
        "else:\n",
        "    print(f\"Tweet: '{new_tweet}' is predicted as a fake disaster.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YFnokbo3Y9m",
        "outputId": "9b6f01a6-6d2f-45db-dba9-10446f197eba"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet: 'There's a huge fire near my house, and people are evacuating!' is predicted as a real disaster.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7sqz60M7_7qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5vzZS7HN3ogt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GTjU3Qpp3tPW"
      }
    }
  ]
}